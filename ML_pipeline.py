# -*- coding: utf-8 -*-
"""hope.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e5khy7fTz-DBIt8MuGgRqOU048FDGKJ7
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re, string, os, warnings
warnings.filterwarnings('ignore')

from bs4 import BeautifulSoup
from collections import Counter
from wordcloud import WordCloud
from textblob import TextBlob

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier
from sklearn.svm import LinearSVC
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
from scipy.sparse import hstack
import joblib

"""#LOADING DATASET FROM KAGGLE

"""

import kagglehub
path = kagglehub.dataset_download("clmentbisaillon/fake-and-real-news-dataset")
print("Path to dataset files:", path)

fake_df = pd.read_csv('/content/Fake.csv')
true_df = pd.read_csv('/content/True.csv')

fake_df['label'] = 'fake'
true_df['label'] = 'real'

df = pd.concat([fake_df, true_df], axis=0).reset_index(drop=True)
print("Dataset shape before cleaning:", df.shape)

!zip model_files.zip ensemble_model.pkl tfidf_vectorizer.pkl numeric_scaler.pkl

from google.colab import files
files.download("model_files.zip")



"""#data exploration"""

df.head()

df.tail()

df.isna().sum()

df.info()

print("Duplicate entries:", df.duplicated().sum())

#add combined col
df['combined_text'] = df['title'].fillna('') + ' ' + df['text'].fillna('')
df = df.drop(columns=['date', 'title', 'text'], errors='ignore')
df = df.drop_duplicates()

#label distribution
sns.countplot(data=df, x='label', palette='cool')
plt.title('Label Distribution (Fake vs Real)')
plt.show()

"""#wordcloud"""

# Wordcloud visualization for fake and real news
fake_text = ' '.join(df[df['label']=='fake']['combined_text'][:500])
real_text = ' '.join(df[df['label']=='real']['combined_text'][:500])
fig, ax = plt.subplots(1, 2, figsize=(15, 6))

fake_wc = WordCloud(width=600, height=400, background_color='white', colormap='Reds').generate(fake_text)
real_wc = WordCloud(width=600, height=400, background_color='white', colormap='Blues').generate(real_text)

ax[0].imshow(fake_wc, interpolation='bilinear')
ax[0].set_title("Fake News WordCloud", fontsize=14)
ax[0].axis("off")

ax[1].imshow(real_wc, interpolation='bilinear')
ax[1].set_title("Real News WordCloud", fontsize=14)
ax[1].axis("off")

plt.tight_layout()
plt.show()


print("Dataset shape after cleaning:", df.shape)

"""#Cleanign & Preprocessing"""

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
nltk.download('stopwords', quiet=True)

stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()

def preprocess_text_stemming(text):
    if pd.isna(text) or not isinstance(text, str):
        return ""
    soup = BeautifulSoup(text, 'html.parser')
    text = soup.get_text(separator=' ')
    text = re.sub(r'http\S+|www\S+', '', text)
    text = text.lower()
    tokens = [stemmer.stem(w) for w in text.split() if w.isalpha() and w not in stop_words]
    return ' '.join(tokens)

df['cleaned_text'] = [preprocess_text_stemming(t) for t in df['combined_text']]
print("Preprocessing complete.")

"""#SAVING CLEANED DATASET"""

# Save the final cleaned DataFrame to a CSV file
df.to_csv('fake_news.csv', index=False)

"""#EDA"""

# df1 = pd.read_csv('fake_news.csv')
# df_shuffled = df1.sample(frac=1, random_state=42).reset_index(drop=True)
# df_shuffled.head(10)

df.columns

# EDA: Most common words for fake and real
real_words = ' '.join(df[df['label']=='real']['cleaned_text'])
fake_words = ' '.join(df[df['label']=='fake']['cleaned_text'])

def plot_top_words(text, title, n=20):
    words = text.split()
    counter = Counter(words)
    common = counter.most_common(n)
    terms, freqs = zip(*common)
    plt.figure(figsize=(10,5))
    sns.barplot(x=list(freqs), y=list(terms), palette='viridis')
    plt.title(title)
    plt.show()

plot_top_words(real_words, 'Top Real News Words')
plot_top_words(fake_words, 'Top Fake News Words')

df.shape

"""#FEATURE ENGG"""

def extract_features(text):
    text = str(text)
    chars = len(text)
    words = text.split()
    word_count = len(words)
    avg_word_len = np.mean([len(w) for w in words]) if word_count > 0 else 0
    unique_ratio = len(set(words)) / (word_count + 1)
    punct_count = sum(1 for c in text if c in string.punctuation)
    num_exclam = text.count('!')
    num_digits = sum(1 for c in text if c.isdigit())
    blob = TextBlob(text)
    polarity = blob.sentiment.polarity
    subjectivity = blob.sentiment.subjectivity
    return [chars, word_count, avg_word_len, unique_ratio, punct_count, num_exclam, num_digits, polarity, subjectivity]

feature_names = ['char_count','word_count','avg_word_len','unique_ratio','punct_count','num_exclam','num_digits','polarity','subjectivity']
feature_df = pd.DataFrame(df['cleaned_text'].apply(lambda x: extract_features(x)).tolist(), columns=feature_names)

# # Visualize feature distributions
# feature_df[['char_count','word_count','avg_word_len','punct_count']].hist(bins=30, figsize=(10,8), color='#5DADE2')
# plt.suptitle('Feature Distributions')
# plt.show()

# print("Feature engineering complete.")

# Visualize feature distributions with axis labels
axes = feature_df[['char_count','word_count','avg_word_len','punct_count']].hist(
    bins=30, figsize=(10,8), color='#5DADE2'
)

# Add axis labels manually for each subplot
feature_labels = {
    'char_count': ('Character Count', 'Number of Articles'),
    'word_count': ('Word Count', 'Number of Articles'),
    'avg_word_len': ('Average Word Length', 'Number of Articles'),
    'punct_count': ('Punctuation Count', 'Number of Articles')
}

for ax, feature in zip(axes.flatten(), feature_labels):
    ax.set_xlabel(feature_labels[feature][0])
    ax.set_ylabel(feature_labels[feature][1])

plt.suptitle('Feature Distributions')
plt.tight_layout()
plt.show()

"""#TF-IDF + FEATURE COMBINATON"""

tfidf = TfidfVectorizer(ngram_range=(1,2), max_df=0.8, min_df=5, max_features=20000)
X_text = tfidf.fit_transform(df['cleaned_text'])
scaler = StandardScaler()
X_num = scaler.fit_transform(feature_df)
X = hstack([X_text, X_num])
y = df['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
print("Combined feature matrix:", X.shape)

"""# SOFT VOTING ENSEMBLE MODEL(LOGISTIC REGRESSION + RANDOM FOREST)"""

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay


# Logistic Regression (LR)
log_reg = LogisticRegression(
    solver='liblinear',
    random_state=42,
    class_weight='balanced',
    C=0.1
)

# Random Forest (RF)
rand_forest = RandomForestClassifier(
    n_estimators=150,
    max_depth=15,
    min_samples_leaf=10,
    max_features='sqrt',
    random_state=42,
    n_jobs=-1,
    class_weight='balanced'
)

# Soft voting ensemble creatiion
ensemble = VotingClassifier(
    estimators=[
        ('lr', log_reg),
        ('rf', rand_forest)
    ],
    voting='soft',
    n_jobs=-1
)

# Store models
models = {
    'Logistic Regression': log_reg,
    'Random Forest': rand_forest,
    'Soft Ensemble': ensemble
}

results = {}

print("Models Initialized with Tuned Parameters (C=0.1, RF max_depth=15, min_samples_leaf=10).")
print("---" * 25)

# Training and evaluation
for name, model in models.items():
    print(f"\n{'='*15} {name} {'='*15}")

    model.fit(X_train, y_train)

    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    train_acc = accuracy_score(y_train, y_train_pred)
    test_acc = accuracy_score(y_test, y_test_pred)

    results[name] = {'Train Accuracy': train_acc, 'Test Accuracy': test_acc}

    print(f"Train Accuracy: {train_acc:.4f}")
    print(f"Test Accuracy : {test_acc:.4f}")

    print("\nClassification Report (Test):")
    print(classification_report(y_test, y_test_pred))

    if name == 'Soft Ensemble':
        cm = confusion_matrix(y_test, y_test_pred)
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
        disp.plot(cmap=plt.cm.Blues)
        plt.title(f"{name} Confusion Matrix")
        plt.show()

# Summaries of accuracies
print("\n\n==== Summary of All Model Accuracies ====")
print("| Model | Training Accuracy | Testing Accuracy |")
for name, acc in results.items():
    print(f"| {name:<20} | {acc['Train Accuracy']:.4f} | {acc['Test Accuracy']:.4f} |")

train_color = "#6A5ACD"
test_color  = "#A8E6CF"

models_list = list(results.keys())
train_accs = [results[m]['Train Accuracy'] for m in models_list]
test_accs = [results[m]['Test Accuracy'] for m in models_list]

x = np.arange(len(models_list))
width = 0.35

plt.figure(figsize=(10, 6))

plt.bar(x - width/2, train_accs, width, label='Train Accuracy', color=train_color)
plt.bar(x + width/2, test_accs, width, label='Test Accuracy', color=test_color)

plt.ylabel('Accuracy', fontsize=12)
plt.title('Model Accuracy Comparison', fontsize=15, fontweight='bold')
plt.xticks(x, models_list, fontsize=11)
plt.ylim(0, 1)
plt.legend()

plt.grid(axis='y', linestyle='--', alpha=0.4)

plt.show()

from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

print("\n=== 5-Fold Cross Validation for Soft Ensemble ===")

cv_scores = cross_val_score(ensemble, X, y, cv=5, scoring='accuracy')

print("Fold Accuracies :", cv_scores)
print("Mean CV Accuracy:", cv_scores.mean())
print("Std Deviation   :", cv_scores.std())

print("\n=== Confusion Matrix - Soft Ensemble ===")

y_pred_ens = ensemble.predict(X_test)

cm = confusion_matrix(y_test, y_pred_ens, labels=['real', 'fake'])

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Real', 'Fake'])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix - Soft Ensemble Model")
plt.show()



"""#Hard Voting Ensemble"""

from sklearn.svm import LinearSVC
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix


svm = LinearSVC(C=0.01, random_state=42)
pac = PassiveAggressiveClassifier(max_iter=1000, random_state=42,C= 0.001)
rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    random_state=42,
    n_jobs=-1  # Use all CPU cores
)

# Ensemble model (hard voting)
hard_ensemble = VotingClassifier(
    estimators=[
        ('svm', svm),
        ('pac', pac),
        ('rf', rf)
    ],
    voting='hard'
)

# Store models for comparison
models = {
    'SVM': svm,
    'Passive Aggressive': pac,
    'Random Forest': rf,
    'Ensemble': hard_ensemble
}

# Dictionary to store results
results = {}

# Train and evaluate models
for name, model in models.items():
    print(f"\n{'='*20} {name} {'='*20}")
    model.fit(X_train, y_train)

    # Predictions on both train and test sets
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    # Accuracy metrics
    train_acc = accuracy_score(y_train, y_train_pred)
    test_acc = accuracy_score(y_test, y_test_pred)

    results[name] = {'Train Accuracy': train_acc, 'Test Accuracy': test_acc}

    print(f"Train Accuracy: {train_acc:.4f}")
    print(f"Test Accuracy : {test_acc:.4f}")
    print("\nClassification Report (Test):")
    print(classification_report(y_test, y_test_pred))

    # Optional: Confusion matrix (to analyze misclassifications)
    cm = confusion_matrix(y_test, y_test_pred)
    print("Confusion Matrix:\n", cm)

# Print summary of all model accuracies
print("\n\n==== Summary of All Models ====")
for name, acc in results.items():
    print(f"{name:<20} | Train: {acc['Train Accuracy']:.4f} | Test: {acc['Test Accuracy']:.4f}")

# Model comparison chart (train vs test)
plt.figure(figsize=(8,5))
bar_data = pd.DataFrame(results).T
bar_data.plot(kind='bar', figsize=(10,6), color=['#5DADE2', '#48C9B0'])
plt.title('Train vs Test Accuracy Comparison')
plt.ylabel('Accuracy')
plt.xticks(rotation=0)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.legend(title='Accuracy Type')
plt.tight_layout()
plt.show()

cv_scores = cross_val_score(hard_ensemble, X, y, cv=5, scoring='accuracy')
print("5-Fold Cross Validation Accuracy:", cv_scores.mean())

# Confusion matrix for Ensemble
y_pred_ens = hard_ensemble.predict(X_test)
cm = confusion_matrix(y_test, y_pred_ens, labels=['real','fake'])
ConfusionMatrixDisplay(cm, display_labels=['Real','Fake']).plot(cmap='Blues')
plt.title('Confusion Matrix - Ensemble Model')
plt.show()

"""#Soft ensemble testing"""

print("\nSAMPLE TEST CASES\n")

sample_texts = [
    'Authorities in Nigeria are increasing efforts to tackle investment scams, especially those involving crypto platforms.',
    'Breaking News: Trump becomes the president of india',
    'Modi is the Prime Minister of India',
    'Draupadi Murmu is the President of India',
    'U.S., North Korea clash at U.N. arms forum on nuclear threat',
    'Watch This Awesome Mashup of Michael Flynn Leading The ‘Lock Her Up’ Chant As He Goes Off To Court (VIDEO) Donald Trump s disgraced National Security Adviser Michael Flynn has just plead guilty to lying to the FBI   a felony. He has also agreed to testify against Trump in exchange for leniency from Special Counsel Robert Mueller s team. The irony here is beyond delicious   especially since Flynn infamously led the  LOCK HER UP!  chants at the 2016 Republican National Convention, saying that Hillary Clinton was some kind of criminal, and that if Trump was elected they d be able to put her in jail, where many Trump supporters believe she belongs. Well, now the tables are turned, and it is Flynn who will be heading to jail, and the people who realize who the REAL criminals are have been having a field day. Perhaps one of the best pieces of Twitter schaudenfraude is this video of Flynn heading into court to plead guilty with the  Lock her up!  chant being played:I mashed up Michael Flynn s perp walk with audio of him leading a  lock her up  chant. pic.twitter.com/L1o5CjJXrQ  Adam Smith (@asmith83) December 1, 2017This is BEYOND awesome. These fools thought they d get a chance to put Hillary Clinton in jail as if we live in some kind of banana republic. Instead, they are all turning on each other in order to save their own asses in the best circular firing squad any of us ever could have imagined. Michael Flynn is going to sing like a canary so that he can keep himself and his equally criminal son out of federal prison   and railroad the entire Trump crime family into the slammer   just where they belong.The GOP made a deal with the devil when their ignorant, bigoted voters chose this unfit orange overlord to be their presidential nominee. Now, they are very likely to rue the day they ever heard the name Donald Trump.Featured image via Chip Somodevilla/Getty Images",watch awesom mashup michael flynn lead chant goe court donald trump disgrac nation secur advis michael flynn plead guilti lie fbi also agre testifi trump exchang lenienc special counsel robert mueller ironi beyond delici especi sinc flynn infam led lock chant republican nation say hillari clinton kind trump elect abl put mani trump support believ tabl flynn head peopl realiz real crimin field perhap one best piec twitter schaudenfraud video flynn head court plead guilti lock chant mash michael flynn perp walk audio lead lock adam smith decemb beyond fool thought get chanc put hillari clinton jail live kind banana turn order save ass best circular fire squad us ever could michael flynn go sing like canari keep equal crimin son feder prison railroad entir trump crime famili slammer gop made deal devil bigot voter chose unfit orang overlord presidenti like rue day ever heard name donald imag via chip imag',
    'The government successfully passed a new education reform bill today, aiming to improve access to schools and increase funding for teachers. Lawmakers and citizens praised the initiative as a major step forward for the country’s future.'
]

# Function to preprocess, extract features, and predict using the ensemble model
def predict_texts(texts):
    for text in texts:
        cleaned = preprocess_text_stemming(text)
        feats = extract_features(cleaned)
        X_tfidf = tfidf.transform([cleaned])
        X_num = scaler.transform([feats])
        X_comb = hstack([X_tfidf, X_num])
        pred = ensemble.predict(X_comb)[0]
        print(f"News: {text[:100]}...")
        print(f"Predicted Label: {pred.upper()}\n")

# Run the predictions
predict_texts(sample_texts)

"""#SAVING THE ENEMBLE MODEL"""

joblib.dump(ensemble, 'ensemble_model.pkl')
joblib.dump(tfidf, 'tfidf_vectorizer.pkl')
joblib.dump(scaler, 'numeric_scaler.pkl')
print("Saved model and vectorizer artifacts.")

from google.colab import drive
drive.mount('/content/drive')

"""#BERT"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from torch.optim import AdamW
from transformers import AutoModel, BertTokenizerFast
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import kagglehub
import gc

# GPU/CPU setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

#data loding and preprocesisng
print("Downloading dataset from Kaggle...")
path = kagglehub.dataset_download("clmentbisaillon/fake-and-real-news-dataset")

fake_df = pd.read_csv(f'{path}/Fake.csv')
true_df = pd.read_csv(f'{path}/True.csv')

fake_df['label'] = 1
true_df['label'] = 0

data = pd.concat([fake_df, true_df], axis=0).sample(frac=1, random_state=2018).reset_index(drop=True)
data['combined_text'] = data['title'].fillna('') + ' ' + data['text'].fillna('')

initial_rows = data.shape[0]
data.drop_duplicates(subset=['combined_text'], inplace=True)
final_rows = data.shape[0]
data.reset_index(drop=True, inplace=True)

print(f"Initial rows: {initial_rows}, Duplicates removed: {initial_rows - final_rows}")
print(f"Final Shape: {data.shape}")
print(f"Label distribution:\n{data['label'].value_counts()}")


TEXT_COLUMN = data['combined_text']
LABEL_COLUMN = data['label']

train_text, temp_text, train_labels, temp_labels = train_test_split(
    TEXT_COLUMN, LABEL_COLUMN, random_state=2018, test_size=0.3, stratify=LABEL_COLUMN
)
val_text, test_text, val_labels, test_labels = train_test_split(
    temp_text, temp_labels, random_state=2018, test_size=0.5, stratify=temp_labels
)


bert = AutoModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')

MAX_LENGTH = 15

def tokenize(texts):
    tokens = tokenizer.batch_encode_plus(
        texts.tolist(), max_length=MAX_LENGTH, padding='max_length', truncation=True
    )
    seq = torch.tensor(tokens['input_ids'])
    mask = torch.tensor(tokens['attention_mask'])
    return seq, mask

train_seq, train_mask = tokenize(train_text)
val_seq, val_mask = tokenize(val_text)
test_seq, test_mask = tokenize(test_text)

train_y = torch.tensor(train_labels.tolist())
val_y = torch.tensor(val_labels.tolist())
test_y = torch.tensor(test_labels.tolist())

batch_size = 32

train_data = TensorDataset(train_seq, train_mask, train_y)
train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=batch_size)

val_data = TensorDataset(val_seq, val_mask, val_y)
val_dataloader = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size=batch_size)


for param in bert.parameters():
    param.requires_grad = False

class BERT_Arch(nn.Module):
    def __init__(self, bert):
        super(BERT_Arch, self).__init__()
        self.bert = bert
        self.dropout = nn.Dropout(0.1)
        self.relu = nn.ReLU()
        self.fc1 = nn.Linear(768, 512)
        self.fc2 = nn.Linear(512, 2)
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, sent_id, mask):
        cls_hs = self.bert(sent_id.to(device), attention_mask=mask.to(device))['pooler_output']
        x = self.fc1(cls_hs)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        x = self.softmax(x)
        return x

model = BERT_Arch(bert).to(device)

#optimizer loss and traingn setup
optimizer = AdamW(model.parameters(), lr=1e-5)
criterion = nn.NLLLoss()
epochs = 3
weights_save_path = 'c2_new_model_weights.pt'

#train and eval functions
def train():
    model.train()
    total_loss = 0
    for step, batch in enumerate(train_dataloader):
        sent_id, mask, labels = batch
        model.zero_grad()
        preds = model(sent_id, mask)
        loss = criterion(preds, labels.to(device))
        total_loss += loss.item()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        del sent_id, mask, labels, preds, loss
        if device.type == 'cuda': torch.cuda.empty_cache()
    return total_loss / len(train_dataloader)

def evaluate():
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for batch in val_dataloader:
            sent_id, mask, labels = batch
            preds = model(sent_id, mask)
            loss = criterion(preds, labels.to(device))
            total_loss += loss.item()
            del sent_id, mask, labels, preds, loss
            if device.type == 'cuda': torch.cuda.empty_cache()
    return total_loss / len(val_dataloader)

# =======================================================

def get_accuracy(dataloader):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for sent_id, mask, labels in dataloader:
            preds = model(sent_id, mask)
            preds = torch.argmax(preds, dim=1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    return accuracy_score(all_labels, all_preds)

#traingn loop
best_valid_loss = float('inf')
for epoch in range(epochs):
    print(f'\nEpoch {epoch + 1}/{epochs}')
    train_loss = train()
    val_loss = evaluate()

    if val_loss < best_valid_loss:
        best_valid_loss = val_loss
        torch.save(model.state_dict(), weights_save_path)
        print("Model weights saved.")

    print(f"Training Loss: {train_loss:.3f} | Validation Loss: {val_loss:.3f}")


train_acc = get_accuracy(train_dataloader)
val_acc = get_accuracy(val_dataloader)
print("\nTrain Accuracy:", train_acc)
print("Validation Accuracy:", val_acc)


def predict_news_type(texts):
    tokens_inf = tokenizer.batch_encode_plus(
        texts, max_length=MAX_LENGTH, padding='max_length', truncation=True
    )
    seq = torch.tensor(tokens_inf['input_ids'])
    mask = torch.tensor(tokens_inf['attention_mask'])
    with torch.no_grad():
        preds = model(seq, mask)
        predicted_labels = torch.argmax(preds, dim=1).cpu().numpy()
    return predicted_labels


test_cases = [
    "Authorities in Nigeria are increasing efforts to tackle investment scams, especially those involving crypto platforms.",
    "India Women Cricket Team won the Worldcup in 2025",
    "Bomb blast in Delhi on 10 November 2025",
    "Modi is the Prime Minister of India",
    'U.S., North Korea clash at U.N. arms forum on nuclear threat',
    'Watch This Awesome Mashup of Michael Flynn Leading The ‘Lock Her Up’ Chant As He Goes Off To Court (VIDEO) Donald Trump s disgraced National Security Adviser Michael Flynn has just plead guilty to lying to the FBI   a felony. He has also agreed to testify against Trump in exchange for leniency from Special Counsel Robert Mueller s team. The irony here is beyond delicious   especially since Flynn infamously led the  LOCK HER UP!  chants at the 2016 Republican National Convention, saying that Hillary Clinton was some kind of criminal, and that if Trump was elected they d be able to put her in jail, where many Trump supporters believe she belongs. Well, now the tables are turned, and it is Flynn who will be heading to jail, and the people who realize who the REAL criminals are have been having a field day. Perhaps one of the best pieces of Twitter schaudenfraude is this video of Flynn heading into court to plead guilty with the  Lock her up!  chant being played:I mashed up Michael Flynn s perp walk with audio of him leading a  lock her up  chant. pic.twitter.com/L1o5CjJXrQ  Adam Smith (@asmith83) December 1, 2017This is BEYOND awesome. These fools thought they d get a chance to put Hillary Clinton in jail as if we live in some kind of banana republic. Instead, they are all turning on each other in order to save their own asses in the best circular firing squad any of us ever could have imagined. Michael Flynn is going to sing like a canary so that he can keep himself and his equally criminal son out of federal prison   and railroad the entire Trump crime family into the slammer   just where they belong.The GOP made a deal with the devil when their ignorant, bigoted voters chose this unfit orange overlord to be their presidential nominee. Now, they are very likely to rue the day they ever heard the name Donald Trump.Featured image via Chip Somodevilla/Getty Images",watch awesom mashup michael flynn lead chant goe court donald trump disgrac nation secur advis michael flynn plead guilti lie fbi also agre testifi trump exchang lenienc special counsel robert mueller ironi beyond delici especi sinc flynn infam led lock chant republican nation say hillari clinton kind trump elect abl put mani trump support believ tabl flynn head peopl realiz real crimin field perhap one best piec twitter schaudenfraud video flynn head court plead guilti lock chant mash michael flynn perp walk audio lead lock adam smith decemb beyond fool thought get chanc put hillari clinton jail live kind banana turn order save ass best circular fire squad us ever could michael flynn go sing like canari keep equal crimin son feder prison railroad entir trump crime famili slammer gop made deal devil bigot voter chose unfit orang overlord presidenti like rue day ever heard name donald imag via chip imag',
    'The government successfully passed a new education reform bill today, aiming to improve access to schools and increase funding for teachers. Lawmakers and citizens praised the initiative as a major step forward for the country’s future.'
]

predictions = predict_news_type(test_cases)
label_map = {1: "FAKE NEWS", 0: "REAL NEWS"}
for text, pred in zip(test_cases, predictions):
    print(f"Text: '{text}'\nPrediction: {label_map[pred]}\n")


del model, bert, tokenizer
gc.collect()
if device.type == 'cuda': torch.cuda.empty_cache()







